{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Matching Bags From Shoe Images And Vice Versa Using DiscoGANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from the link  given bellol edges2handbags:https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz\n",
    "edges2shoes:https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0002\n",
    "#EPOCHS = 100000\n",
    "EPOCHS = 101\n",
    "RESTORE_TRAINING= False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in these datasets contain two sub-images. One is the colored image of the object, while the other is the image of the edges of the corresponding color image.\n",
    "\n",
    "Resize and crop the images in the dataset to obtain the handbag and shoe images, and save the images in the corresponding folders of bags and shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files(data_dir,type = 'bags'):\n",
    "    '''\n",
    "    :param data_dir: Input directory\n",
    "    :param type: bags or shoes\n",
    "    :return: saves the cropped files to the bags to shoes directory\n",
    "    '''\n",
    "    input_file_dir = os.path.join(os.getcwd(),data_dir, \"train\")\n",
    "    result_dir = os.path.join(os.getcwd(),type)\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    file_names= os.listdir(input_file_dir)\n",
    "    count = 0\n",
    "    for file in file_names:\n",
    "        input_image = Image.open(os.path.join(input_file_dir,file))\n",
    "        input_image = input_image.resize([128, 64])\n",
    "        input_image = input_image.crop([64, 0, 128, 64])  # Cropping only the colored image. Excluding the edge image\n",
    "        input_image.save(os.path.join(result_dir,file))\n",
    "        count = count+1\n",
    "        if(count%1000==0):\n",
    "            print(\"extract files \"+str(count)+\" in dataset \"+type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    '''\n",
    "    Before executing this function. Follow these steps;\n",
    "    1. Download the datasets\n",
    "    Handbags data Link 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz'\n",
    "    Shoes data Link 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz'\n",
    "    2. Extract the tar files.\n",
    "    3. Execute this function. This function will extract the handbags and shoe images from the datasets.\n",
    "    '''\n",
    "    extract_files(\"edges2shoes\", 'shoes')\n",
    "    extract_files(\"edges2handbags\", 'bags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_type = 'train'):\n",
    "    shoelist = glob.glob(os.path.join(os.getcwd(), \"shoes/*jpg\"))\n",
    "    shoe_data = np.array([np.array(Image.open(fname)) for fname in shoelist]).astype(np.float32)\n",
    "    baglist = glob.glob(os.path.join(os.getcwd(), \"bags/*jpg\"))\n",
    "    bags_data = np.array([np.array(Image.open(fname)) for fname in baglist]).astype(np.float32)\n",
    "    shoe_data = shoe_data/255.\n",
    "    bags_data = bags_data/255.\n",
    "    return shoe_data, bags_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(global_step, img_data, file_name):\n",
    "    sample_results_dir = os.path.join(os.getcwd(), \"sample_results\", \"epoch_\" +str(global_step))\n",
    "    if not os.path.exists(sample_results_dir):\n",
    "        os.makedirs(sample_results_dir)\n",
    "    result = Image.fromarray((img_data[0] * 255).astype(np.uint8))\n",
    "    result.save(os.path.join(sample_results_dir, file_name + \".jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator has 5 convolutional layers followed by two fully connected layers with Leaky ReLU activation is used for all layers except the last fully connected layer and The last layer uses sigmoid to predict the probability of a sample.\n",
    "Batch normalization is used, except on the first and last layers of the network and stride length of 2 is used for all of the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x,initializer, scope_name ='discriminator',  reuse=False):\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=x, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, weights_initializer=initializer,\n",
    "                                         scope=\"disc_conv1\")  # 32 x 32 x 32\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv2\")  # 16 x 16 x 64\n",
    "        conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv3\")  # 8 x 8 x 128\n",
    "        conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv4\")  # 4 x 4 x 256\n",
    "        conv5 = tf.contrib.layers.conv2d(inputs=conv4, num_outputs=512, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv5\")  # 2 x 2 x 512\n",
    "        fc1 = tf.reshape(conv5, shape=[tf.shape(x)[0], 2 * 2 * 512])\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512, reuse=reuse, activation_fn=tf.nn.leaky_relu,\n",
    "                                                normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                                weights_initializer=initializer, scope=\"disc_fc1\")\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=reuse, activation_fn=tf.nn.sigmoid,\n",
    "                                                weights_initializer=initializer, scope=\"disc_fc2\")\n",
    "\n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generator network has 4 convolutional layers followed by 4 convolutional transpose (or deconv) layers with kernel size is 4 and stride is 2 and 1 for the convolutional and deconv layers, respectively. Leaky Relu is used as activation function in all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x, initializer, scope_name = 'generator',reuse=False):\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=x, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, weights_initializer=initializer,\n",
    "                                         scope=\"disc_conv1\")  # 32 x 32 x 32\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv2\")  # 16 x 16 x 64\n",
    "        conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv3\")  # 8 x 8 x 128\n",
    "        conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\",\n",
    "                                         reuse=reuse, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                         weights_initializer=initializer, scope=\"disc_conv4\")  # 4 x 4 x 256\n",
    "\n",
    "        deconv1 = tf.contrib.layers.conv2d(conv4, num_outputs=4 * 128, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv1\")\n",
    "        deconv1 = tf.reshape(deconv1, shape=[tf.shape(x)[0], 8, 8, 128])\n",
    "\n",
    "        deconv2 = tf.contrib.layers.conv2d(deconv1, num_outputs=4 * 64, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv2\")\n",
    "        deconv2 = tf.reshape(deconv2, shape=[tf.shape(x)[0], 16, 16, 64])\n",
    "\n",
    "        deconv3 = tf.contrib.layers.conv2d(deconv2, num_outputs=4 * 32, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv3\")\n",
    "        deconv3 = tf.reshape(deconv3, shape=[tf.shape(x)[0], 32, 32, 32])\n",
    "\n",
    "        deconv4 = tf.contrib.layers.conv2d(deconv3, num_outputs=4 * 16, kernel_size=4, stride=1, padding=\"SAME\",\n",
    "                                               activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                               weights_initializer=initializer, scope=\"gen_conv4\")\n",
    "        deconv4 = tf.reshape(deconv4, shape=[tf.shape(x)[0], 64, 64, 16])\n",
    "\n",
    "        recon = tf.contrib.layers.conv2d(deconv4, num_outputs=3, kernel_size=4, stride=1, padding=\"SAME\", \\\n",
    "                                             activation_fn=tf.nn.relu, scope=\"gen_conv5\")\n",
    "\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define_network function is used to defines the two generators and two discriminator for each domain. However, for DiscoGANs, the function defines one generator that generates fake images in another domain, and one generator that does the reconstruction. Also, the discriminators are defined for both real and fake images in each domain.\n",
    "\n",
    "define_loss function defines the reconstruction loss based on the Euclidean distance between the reconstructed and original image. To generate the GAN and discriminator loss, the function uses the cross entropy function.\n",
    "\n",
    "define_optimizer function use AdamOptimizer as optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscoGAN:\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope('Input'):\n",
    "            self.X_bags = tf.placeholder(shape = [None, 64, 64, 3], name='bags', dtype=tf.float32)\n",
    "            self.X_shoes = tf.placeholder(shape= [None, 64, 64, 3], name='shoes',dtype= tf.float32)\n",
    "        self.initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "        self.define_network()\n",
    "        self.define_loss()\n",
    "        self.get_trainable_params()\n",
    "        self.define_optimizer()\n",
    "        self.summary_()\n",
    "\n",
    "    def define_network(self):\n",
    "        \n",
    "        # Generators\n",
    "        # This one is used to generate fake data\n",
    "        self.gen_b_fake = generator(self.X_shoes, self.initializer,scope_name=\"generator_sb\")\n",
    "        self.gen_s_fake =   generator(self.X_bags, self.initializer,scope_name=\"generator_bs\")\n",
    "\n",
    "        # Reconstruction Generators\n",
    "        # Note that parameters are being used from previous layers\n",
    "        self.gen_recon_s = generator(self.gen_b_fake, self.initializer,scope_name=\"generator_sb\",  reuse=True)\n",
    "        self.gen_recon_b = generator(self.gen_s_fake,  self.initializer, scope_name=\"generator_bs\", reuse=True)\n",
    "\n",
    "        # Discriminator for Shoes\n",
    "        self.disc_s_real = discriminator(self.X_shoes,self.initializer, scope_name=\"discriminator_s\")\n",
    "        self.disc_s_fake = discriminator(self.gen_s_fake,self.initializer, scope_name=\"discriminator_s\", reuse=True)\n",
    "\n",
    "        # Discriminator for Bags\n",
    "        self.disc_b_real = discriminator(self.X_bags,self.initializer,scope_name=\"discriminator_b\")\n",
    "        self.disc_b_fake = discriminator(self.gen_b_fake, self.initializer, reuse=True,scope_name=\"discriminator_b\")\n",
    "\n",
    "        # Defining Discriminators of Bags and Shoes\n",
    "\n",
    "    def define_loss(self):\n",
    "        # Reconstruction loss for generators\n",
    "        self.const_loss_s = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_s, self.X_shoes))\n",
    "        self.const_loss_b = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_b, self.X_bags))\n",
    "\n",
    "        # Generator loss for GANs\n",
    "        self.gen_s_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.ones_like(self.disc_s_fake)))\n",
    "        self.gen_b_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.ones_like(self.disc_b_fake)))\n",
    "\n",
    "        # Total Generator Loss\n",
    "        self.gen_loss =  (self.const_loss_b + self.const_loss_s)  + self.gen_s_loss + self.gen_b_loss\n",
    "\n",
    "        # Cross Entropy loss for discriminators for shoes and bags\n",
    "        # Shoes\n",
    "        self.disc_s_real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_real, labels=tf.ones_like(self.disc_s_real)))\n",
    "        self.disc_s_fake_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.zeros_like(self.disc_s_fake)))\n",
    "        self.disc_s_loss = self.disc_s_real_loss + self.disc_s_fake_loss  # Combined\n",
    "\n",
    "\n",
    "        # Bags\n",
    "        self.disc_b_real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_real, labels=tf.ones_like(self.disc_b_real)))\n",
    "        self.disc_b_fake_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.zeros_like(self.disc_b_fake)))\n",
    "        self.disc_b_loss = self.disc_b_real_loss + self.disc_b_fake_loss\n",
    "\n",
    "        # Total Discriminator Loss\n",
    "        self.disc_loss = self.disc_b_loss + self.disc_s_loss\n",
    "\n",
    "    def get_trainable_params(self):\n",
    "        '''\n",
    "        This function is useful for obtaining trainable parameters which need to be trained either with discriminator or generator loss\n",
    "        :return:\n",
    "        '''\n",
    "        self.disc_params = []\n",
    "        self.gen_params = []\n",
    "        for var in tf.trainable_variables():\n",
    "            if 'generator' in var.name:\n",
    "                self.gen_params.append(var)\n",
    "            elif 'discriminator' in var.name:\n",
    "                self.disc_params.append(var)\n",
    "\n",
    "    def define_optimizer(self):\n",
    "        self.disc_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.disc_loss, var_list=self.disc_params)\n",
    "        self.gen_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.gen_loss, var_list=self.gen_params)\n",
    "\n",
    "    def summary_(self):\n",
    "        # Store the losses\n",
    "        tf.summary.scalar(\"gen_loss\", self.gen_loss)\n",
    "        tf.summary.scalar(\"gen_s_loss\", self.gen_s_loss)\n",
    "        tf.summary.scalar(\"gen_b_loss\", self.gen_b_loss)\n",
    "        tf.summary.scalar(\"const_loss_s\", self.const_loss_s)\n",
    "        tf.summary.scalar(\"const_loss_b\", self.const_loss_b)\n",
    "        tf.summary.scalar(\"disc_loss\", self.disc_loss)\n",
    "        tf.summary.scalar(\"disc_b_loss\", self.disc_b_loss)\n",
    "        tf.summary.scalar(\"disc_s_loss\", self.disc_s_loss)\n",
    "\n",
    "        # Histograms for all vars\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name, var)\n",
    "\n",
    "        self.summary_ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # Load the data first\n",
    "    # Define a function to load the next batch\n",
    "    # start training\n",
    "\n",
    "    # Define a function to get the data for the next batch\n",
    "    def get_next_batch(BATCH_SIZE, type =\"shoes\"):\n",
    "        if type == \"shoes\":\n",
    "            next_batch_indices = random.sample(range(0, X_shoes.shape[0]), BATCH_SIZE)\n",
    "            batch_data = X_shoes[next_batch_indices,:,:,:]\n",
    "        elif type == \"bags\":\n",
    "            next_batch_indices = random.sample(range(0, X_bags.shape[0]), BATCH_SIZE)\n",
    "            batch_data = X_bags[next_batch_indices, :, :, :]\n",
    "        return batch_data\n",
    "\n",
    "    # Loading the dataset\n",
    "    print (\"Loading Dataset\")\n",
    "    X_shoes, X_bags = load_data(load_type='train')\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        if RESTORE_TRAINING:\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(\"./model\")\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('Model Loaded')\n",
    "            start_epoch = int(str(ckpt.model_checkpoint_path).split('-')[-1].split(\".\")[0])\n",
    "            print (\"Start EPOCH\", start_epoch)\n",
    "        else:\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            tf.global_variables_initializer().run()\n",
    "            if not os.path.exists(\"logs\"):\n",
    "                os.makedirs(\"logs\")\n",
    "            start_epoch = 0\n",
    "\n",
    "        # Starting training from here\n",
    "        train_writer = tf.summary.FileWriter(os.getcwd() + '/logs', graph=sess.graph)\n",
    "        print (\"Starting Training\")\n",
    "        for global_step in range(start_epoch,EPOCHS):\n",
    "            shoe_batch = get_next_batch(BATCH_SIZE,\"shoes\")\n",
    "            bag_batch = get_next_batch(BATCH_SIZE,\"bags\")\n",
    "            feed_dict_batch = {model.X_bags: bag_batch, model.X_shoes: shoe_batch}\n",
    "            op_list = [model.disc_optimizer, model.gen_optimizer, model.disc_loss, model.gen_loss, model.summary_]\n",
    "            _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)\n",
    "            shoe_batch = get_next_batch(BATCH_SIZE, \"shoes\")\n",
    "            bag_batch = get_next_batch(BATCH_SIZE, \"bags\")\n",
    "            feed_dict_batch = {model.X_bags: bag_batch, model.X_shoes: shoe_batch}\n",
    "            _, gen_loss = sess.run([model.gen_optimizer, model.gen_loss], feed_dict=feed_dict_batch)\n",
    "            if global_step%10 ==0:\n",
    "                train_writer.add_summary(summary_,global_step)\n",
    "\n",
    "            \n",
    "            print(\"EPOCH:\" + str(global_step) + \"\\tGenerator Loss: \" + str(gen_loss) + \"\\tDiscriminator Loss: \" + str(disc_loss))\n",
    "\n",
    "\n",
    "            if global_step % 5 == 0:\n",
    "\n",
    "                shoe_sample = get_next_batch(1, \"shoes\")\n",
    "                bag_sample = get_next_batch(1, \"bags\")\n",
    "\n",
    "                ops = [model.gen_s_fake, model.gen_b_fake, model.gen_recon_s, model.gen_recon_b]\n",
    "                gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={model.X_shoes: shoe_sample, model.X_bags: bag_sample})\n",
    "\n",
    "                save_image(global_step, gen_s_fake, str(\"gen_s_fake_\") + str(global_step))\n",
    "                save_image(global_step,gen_b_fake, str(\"gen_b_fake_\") + str(global_step))\n",
    "                save_image(global_step, gen_recon_s, str(\"gen_recon_s_\") + str(global_step))\n",
    "                save_image(global_step, gen_recon_b, str(\"gen_recon_b_\") + str(global_step))\n",
    "\n",
    "            if global_step % 5 == 0:\n",
    "                if not os.path.exists(\"./model\"):\n",
    "                    os.makedirs(\"./model\")\n",
    "                saver.save(sess, \"./model\" + '/model-' + str(global_step) + '.ckpt')\n",
    "                print(\"Saved Model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Dataset\")\n",
    "#generate_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had taken 1000 images of edges2handbags and edges2shoes datasets only to train model, and in training I used EPOCHS = 101, for the better result one can train model with EPOCHS value near about 100000 and with all images edges2handbags and edges2shoes datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the model\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv1/weights:0 is illegal; using generator_sb/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv1/biases:0 is illegal; using generator_sb/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv2/weights:0 is illegal; using generator_sb/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv2/BatchNorm/beta:0 is illegal; using generator_sb/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv3/weights:0 is illegal; using generator_sb/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv3/BatchNorm/beta:0 is illegal; using generator_sb/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv4/weights:0 is illegal; using generator_sb/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/disc_conv4/BatchNorm/beta:0 is illegal; using generator_sb/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv1/weights:0 is illegal; using generator_sb/gen_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv1/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv2/weights:0 is illegal; using generator_sb/gen_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv2/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv3/weights:0 is illegal; using generator_sb/gen_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv3/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv4/weights:0 is illegal; using generator_sb/gen_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv4/BatchNorm/beta:0 is illegal; using generator_sb/gen_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv5/weights:0 is illegal; using generator_sb/gen_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_sb/gen_conv5/biases:0 is illegal; using generator_sb/gen_conv5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv1/weights:0 is illegal; using generator_bs/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv1/biases:0 is illegal; using generator_bs/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv2/weights:0 is illegal; using generator_bs/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv2/BatchNorm/beta:0 is illegal; using generator_bs/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv3/weights:0 is illegal; using generator_bs/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv3/BatchNorm/beta:0 is illegal; using generator_bs/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv4/weights:0 is illegal; using generator_bs/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/disc_conv4/BatchNorm/beta:0 is illegal; using generator_bs/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv1/weights:0 is illegal; using generator_bs/gen_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv1/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv2/weights:0 is illegal; using generator_bs/gen_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv2/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv3/weights:0 is illegal; using generator_bs/gen_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv3/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv4/weights:0 is illegal; using generator_bs/gen_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv4/BatchNorm/beta:0 is illegal; using generator_bs/gen_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv5/weights:0 is illegal; using generator_bs/gen_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name generator_bs/gen_conv5/biases:0 is illegal; using generator_bs/gen_conv5/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv1/weights:0 is illegal; using discriminator_s/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv1/biases:0 is illegal; using discriminator_s/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv2/weights:0 is illegal; using discriminator_s/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv2/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv3/weights:0 is illegal; using discriminator_s/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv3/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv4/weights:0 is illegal; using discriminator_s/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv4/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv5/weights:0 is illegal; using discriminator_s/disc_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_conv5/BatchNorm/beta:0 is illegal; using discriminator_s/disc_conv5/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc1/weights:0 is illegal; using discriminator_s/disc_fc1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc1/BatchNorm/beta:0 is illegal; using discriminator_s/disc_fc1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc2/weights:0 is illegal; using discriminator_s/disc_fc2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_s/disc_fc2/biases:0 is illegal; using discriminator_s/disc_fc2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv1/weights:0 is illegal; using discriminator_b/disc_conv1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv1/biases:0 is illegal; using discriminator_b/disc_conv1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv2/weights:0 is illegal; using discriminator_b/disc_conv2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv2/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv2/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv3/weights:0 is illegal; using discriminator_b/disc_conv3/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv3/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv3/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv4/weights:0 is illegal; using discriminator_b/disc_conv4/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv4/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv4/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv5/weights:0 is illegal; using discriminator_b/disc_conv5/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_conv5/BatchNorm/beta:0 is illegal; using discriminator_b/disc_conv5/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc1/weights:0 is illegal; using discriminator_b/disc_fc1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc1/BatchNorm/beta:0 is illegal; using discriminator_b/disc_fc1/BatchNorm/beta_0 instead.\n",
      "INFO:tensorflow:Summary name discriminator_b/disc_fc2/weights:0 is illegal; using discriminator_b/disc_fc2/weights_0 instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name discriminator_b/disc_fc2/biases:0 is illegal; using discriminator_b/disc_fc2/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "print (\"Defining the model\")\n",
    "model = DiscoGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Loading Dataset\n",
      "Starting Training\n",
      "EPOCH:0\tGenerator Loss: 2.0008836\tDiscriminator Loss: 2.9090877\n",
      "Saved Model\n",
      "EPOCH:1\tGenerator Loss: 1.9483382\tDiscriminator Loss: 2.9022524\n",
      "EPOCH:2\tGenerator Loss: 1.8980455\tDiscriminator Loss: 2.8951457\n",
      "EPOCH:3\tGenerator Loss: 1.8393145\tDiscriminator Loss: 2.889358\n",
      "EPOCH:4\tGenerator Loss: 1.770843\tDiscriminator Loss: 2.8822088\n",
      "EPOCH:5\tGenerator Loss: 1.6854008\tDiscriminator Loss: 2.876473\n",
      "Saved Model\n",
      "EPOCH:6\tGenerator Loss: 1.60859\tDiscriminator Loss: 2.8680203\n",
      "EPOCH:7\tGenerator Loss: 1.5587528\tDiscriminator Loss: 2.8609872\n",
      "EPOCH:8\tGenerator Loss: 1.5108027\tDiscriminator Loss: 2.8532436\n",
      "EPOCH:9\tGenerator Loss: 1.477268\tDiscriminator Loss: 2.8475478\n",
      "EPOCH:10\tGenerator Loss: 1.4469783\tDiscriminator Loss: 2.8373618\n",
      "Saved Model\n",
      "EPOCH:11\tGenerator Loss: 1.4245676\tDiscriminator Loss: 2.8324404\n",
      "EPOCH:12\tGenerator Loss: 1.4040064\tDiscriminator Loss: 2.819888\n",
      "EPOCH:13\tGenerator Loss: 1.3847953\tDiscriminator Loss: 2.8170047\n",
      "EPOCH:14\tGenerator Loss: 1.3794873\tDiscriminator Loss: 2.8081164\n",
      "EPOCH:15\tGenerator Loss: 1.3635923\tDiscriminator Loss: 2.794214\n",
      "Saved Model\n",
      "EPOCH:16\tGenerator Loss: 1.3507408\tDiscriminator Loss: 2.7893195\n",
      "EPOCH:17\tGenerator Loss: 1.3391904\tDiscriminator Loss: 2.7835464\n",
      "EPOCH:18\tGenerator Loss: 1.3302019\tDiscriminator Loss: 2.7662807\n",
      "EPOCH:19\tGenerator Loss: 1.3231809\tDiscriminator Loss: 2.7603164\n",
      "EPOCH:20\tGenerator Loss: 1.314137\tDiscriminator Loss: 2.7600505\n",
      "Saved Model\n",
      "EPOCH:21\tGenerator Loss: 1.3010803\tDiscriminator Loss: 2.7884722\n",
      "EPOCH:22\tGenerator Loss: 1.2988801\tDiscriminator Loss: 2.784419\n",
      "EPOCH:23\tGenerator Loss: 1.3020654\tDiscriminator Loss: 2.777201\n",
      "EPOCH:24\tGenerator Loss: 1.3040849\tDiscriminator Loss: 2.7777278\n",
      "EPOCH:25\tGenerator Loss: 1.3040465\tDiscriminator Loss: 2.7844663\n",
      "Saved Model\n",
      "EPOCH:26\tGenerator Loss: 1.3010387\tDiscriminator Loss: 2.762943\n",
      "EPOCH:27\tGenerator Loss: 1.2928247\tDiscriminator Loss: 2.7606068\n",
      "EPOCH:28\tGenerator Loss: 1.2962534\tDiscriminator Loss: 2.7642446\n",
      "EPOCH:29\tGenerator Loss: 1.2880976\tDiscriminator Loss: 2.7622318\n",
      "EPOCH:30\tGenerator Loss: 1.2924135\tDiscriminator Loss: 2.7611778\n",
      "Saved Model\n",
      "EPOCH:31\tGenerator Loss: 1.2894771\tDiscriminator Loss: 2.762628\n",
      "EPOCH:32\tGenerator Loss: 1.2811534\tDiscriminator Loss: 2.7622423\n",
      "EPOCH:33\tGenerator Loss: 1.279785\tDiscriminator Loss: 2.7632117\n",
      "EPOCH:34\tGenerator Loss: 1.2744355\tDiscriminator Loss: 2.768572\n",
      "EPOCH:35\tGenerator Loss: 1.2759733\tDiscriminator Loss: 2.7955766\n",
      "Saved Model\n",
      "EPOCH:36\tGenerator Loss: 1.2865293\tDiscriminator Loss: 2.811377\n",
      "EPOCH:37\tGenerator Loss: 1.2860769\tDiscriminator Loss: 2.8331332\n",
      "EPOCH:38\tGenerator Loss: 1.2952359\tDiscriminator Loss: 2.8201714\n",
      "EPOCH:39\tGenerator Loss: 1.2914978\tDiscriminator Loss: 2.806261\n",
      "EPOCH:40\tGenerator Loss: 1.28387\tDiscriminator Loss: 2.7974682\n",
      "Saved Model\n",
      "EPOCH:41\tGenerator Loss: 1.263752\tDiscriminator Loss: 2.8410378\n",
      "EPOCH:42\tGenerator Loss: 1.2548108\tDiscriminator Loss: 2.857809\n",
      "EPOCH:43\tGenerator Loss: 1.2609745\tDiscriminator Loss: 2.8597267\n",
      "EPOCH:44\tGenerator Loss: 1.2574666\tDiscriminator Loss: 2.8450694\n",
      "EPOCH:45\tGenerator Loss: 1.2707803\tDiscriminator Loss: 2.8340292\n",
      "Saved Model\n",
      "EPOCH:46\tGenerator Loss: 1.2709104\tDiscriminator Loss: 2.817504\n",
      "EPOCH:47\tGenerator Loss: 1.2705073\tDiscriminator Loss: 2.8205962\n",
      "EPOCH:48\tGenerator Loss: 1.2676809\tDiscriminator Loss: 2.8126793\n",
      "EPOCH:49\tGenerator Loss: 1.2619143\tDiscriminator Loss: 2.8282561\n",
      "EPOCH:50\tGenerator Loss: 1.25039\tDiscriminator Loss: 2.8180435\n",
      "Saved Model\n",
      "EPOCH:51\tGenerator Loss: 1.2332321\tDiscriminator Loss: 2.8208737\n",
      "EPOCH:52\tGenerator Loss: 1.2209605\tDiscriminator Loss: 2.8382792\n",
      "EPOCH:53\tGenerator Loss: 1.2007262\tDiscriminator Loss: 2.8386788\n",
      "EPOCH:54\tGenerator Loss: 1.210386\tDiscriminator Loss: 2.8576157\n",
      "EPOCH:55\tGenerator Loss: 1.213033\tDiscriminator Loss: 2.848805\n",
      "Saved Model\n",
      "EPOCH:56\tGenerator Loss: 1.223701\tDiscriminator Loss: 2.8353736\n",
      "EPOCH:57\tGenerator Loss: 1.2221028\tDiscriminator Loss: 2.8151188\n",
      "EPOCH:58\tGenerator Loss: 1.2186972\tDiscriminator Loss: 2.805352\n",
      "EPOCH:59\tGenerator Loss: 1.228466\tDiscriminator Loss: 2.8131905\n",
      "EPOCH:60\tGenerator Loss: 1.231123\tDiscriminator Loss: 2.8318894\n",
      "Saved Model\n",
      "EPOCH:61\tGenerator Loss: 1.23383\tDiscriminator Loss: 2.842895\n",
      "EPOCH:62\tGenerator Loss: 1.2365305\tDiscriminator Loss: 2.8834255\n",
      "EPOCH:63\tGenerator Loss: 1.2451357\tDiscriminator Loss: 2.876926\n",
      "EPOCH:64\tGenerator Loss: 1.2385839\tDiscriminator Loss: 2.862491\n",
      "EPOCH:65\tGenerator Loss: 1.2460103\tDiscriminator Loss: 2.8747773\n",
      "Saved Model\n",
      "EPOCH:66\tGenerator Loss: 1.234118\tDiscriminator Loss: 2.8916738\n",
      "EPOCH:67\tGenerator Loss: 1.2195301\tDiscriminator Loss: 2.890855\n",
      "EPOCH:68\tGenerator Loss: 1.2135139\tDiscriminator Loss: 2.9063773\n",
      "EPOCH:69\tGenerator Loss: 1.2193213\tDiscriminator Loss: 2.8927202\n",
      "EPOCH:70\tGenerator Loss: 1.2107998\tDiscriminator Loss: 2.89666\n",
      "Saved Model\n",
      "EPOCH:71\tGenerator Loss: 1.1956172\tDiscriminator Loss: 2.8989375\n",
      "EPOCH:72\tGenerator Loss: 1.186748\tDiscriminator Loss: 2.9112654\n",
      "EPOCH:73\tGenerator Loss: 1.1931816\tDiscriminator Loss: 2.9004073\n",
      "EPOCH:74\tGenerator Loss: 1.1854165\tDiscriminator Loss: 2.909259\n",
      "EPOCH:75\tGenerator Loss: 1.1893902\tDiscriminator Loss: 2.893599\n",
      "Saved Model\n",
      "EPOCH:76\tGenerator Loss: 1.1859245\tDiscriminator Loss: 2.8815088\n",
      "EPOCH:77\tGenerator Loss: 1.2257218\tDiscriminator Loss: 2.871328\n",
      "EPOCH:78\tGenerator Loss: 1.2075754\tDiscriminator Loss: 2.8702826\n",
      "EPOCH:79\tGenerator Loss: 1.2172253\tDiscriminator Loss: 2.854199\n",
      "EPOCH:80\tGenerator Loss: 1.2025952\tDiscriminator Loss: 2.8412843\n",
      "Saved Model\n",
      "EPOCH:81\tGenerator Loss: 1.1905917\tDiscriminator Loss: 2.8528728\n",
      "EPOCH:82\tGenerator Loss: 1.1952345\tDiscriminator Loss: 2.8576956\n",
      "EPOCH:83\tGenerator Loss: 1.1923931\tDiscriminator Loss: 2.8464983\n",
      "EPOCH:84\tGenerator Loss: 1.1767931\tDiscriminator Loss: 2.8665352\n",
      "EPOCH:85\tGenerator Loss: 1.1929507\tDiscriminator Loss: 2.8424447\n",
      "Saved Model\n",
      "EPOCH:86\tGenerator Loss: 1.1806228\tDiscriminator Loss: 2.8718889\n",
      "EPOCH:87\tGenerator Loss: 1.1744845\tDiscriminator Loss: 2.8439322\n",
      "EPOCH:88\tGenerator Loss: 1.1714582\tDiscriminator Loss: 2.853804\n",
      "EPOCH:89\tGenerator Loss: 1.1736381\tDiscriminator Loss: 2.832529\n",
      "EPOCH:90\tGenerator Loss: 1.16928\tDiscriminator Loss: 2.852983\n",
      "Saved Model\n",
      "EPOCH:91\tGenerator Loss: 1.1604733\tDiscriminator Loss: 2.8479757\n",
      "EPOCH:92\tGenerator Loss: 1.1730101\tDiscriminator Loss: 2.8445578\n",
      "EPOCH:93\tGenerator Loss: 1.1703565\tDiscriminator Loss: 2.8449094\n",
      "EPOCH:94\tGenerator Loss: 1.1598794\tDiscriminator Loss: 2.8726873\n",
      "EPOCH:95\tGenerator Loss: 1.1415832\tDiscriminator Loss: 2.857943\n",
      "Saved Model\n",
      "EPOCH:96\tGenerator Loss: 1.1496553\tDiscriminator Loss: 2.8726115\n",
      "EPOCH:97\tGenerator Loss: 1.1405507\tDiscriminator Loss: 2.861672\n",
      "EPOCH:98\tGenerator Loss: 1.1573303\tDiscriminator Loss: 2.8542998\n",
      "EPOCH:99\tGenerator Loss: 1.1655511\tDiscriminator Loss: 2.82828\n",
      "EPOCH:100\tGenerator Loss: 1.1500542\tDiscriminator Loss: 2.8068457\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "print (\"Training\")\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
